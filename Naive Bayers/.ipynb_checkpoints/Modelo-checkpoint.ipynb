{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be05b44b-c856-4df6-be7a-34da27dba6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Preparação dos Dados ---\n",
    "# Para este exemplo, vamos criar um conjunto de dados sintético.\n",
    "# Em um cenário real, você carregaria seus dados (por exemplo, de um CSV, banco de dados, etc.).\n",
    "# Vamos criar duas classes e duas características para visualização fácil.\n",
    "\n",
    "# Gerando dados para a Classe 0\n",
    "np.random.seed(42) # Para reprodutibilidade\n",
    "X_class0 = np.random.randn(100, 2) * 1.5 - np.array([2, 2]) # 100 amostras, 2 características, centralizadas em [-2, -2]\n",
    "y_class0 = np.zeros(100) # Rótulo da classe 0\n",
    "\n",
    "# Gerando dados para a Classe 1\n",
    "X_class1 = np.random.randn(100, 2) * 1.5 + np.array([2, 2]) # 100 amostras, 2 características, centralizadas em [2, 2]\n",
    "y_class1 = np.ones(100) # Rótulo da classe 1\n",
    "\n",
    "# Concatenando os dados das duas classes\n",
    "X = np.vstack((X_class0, X_class1)) # Features (características)\n",
    "y = np.hstack((y_class0, y_class1)) # Labels (rótulos das classes)\n",
    "\n",
    "print(\"Shape das características (X):\", X.shape) # Ex: (200, 2) -> 200 amostras, 2 características\n",
    "print(\"Shape dos rótulos (y):\", y.shape)     # Ex: (200,)   -> 200 rótulos\n",
    "\n",
    "# O próximo passo é crucial: dividir os dados em conjuntos de treino e teste.\n",
    "# Isso garante que avaliaremos o modelo em dados que ele \"nunca viu\" durante o treinamento.\n",
    "# test_size=0.3 significa que 30% dos dados serão para teste e 70% para treino.\n",
    "# random_state garante que a divisão seja a mesma cada vez que você executar o código.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"\\n--- Dados Após Divisão ---\")\n",
    "print(\"Shape de X_train:\", X_train.shape)\n",
    "print(\"Shape de X_test:\", X_test.shape)\n",
    "print(\"Shape de y_train:\", y_train.shape)\n",
    "print(\"Shape de y_test:\", y_test.shape)\n",
    "\n",
    "\n",
    "# 2. Treinamento do Modelo (Fase de Aprendizado)\n",
    "# Agora, vamos inicializar e treinar o algoritmo Naive Bayes.\n",
    "# Utilizaremos o Gaussian Naive Bayes, que assume que as características\n",
    "# seguem uma distribuição Gaussiana (normal) dentro de cada classe.\n",
    "# É adequado para dados numéricos contínuos.\n",
    "\n",
    "# Inicializando o classificador Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Treinando o modelo usando os dados de treinamento (X_train e y_train).\n",
    "# Neste passo, o algoritmo calcula:\n",
    "# 1. As probabilidades a priori de cada classe (P(C)).\n",
    "#    Ex: Qual a porcentagem de amostras da classe 0 e da classe 1 no treino?\n",
    "# 2. As probabilidades condicionais (P(X|C)) para cada característica dentro de cada classe.\n",
    "#    Para Gaussian Naive Bayes, isso significa calcular a média e o desvio padrão\n",
    "#    de cada característica para cada classe.\n",
    "print(\"\\n--- Treinando o Modelo Gaussian Naive Bayes ---\")\n",
    "gnb.fit(X_train, y_train)\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "\n",
    "# --- 3. Classificação de Novos Dados (Fase de Predição) ---\n",
    "# Com o modelo treinado, podemos agora usá-lo para fazer previsões\n",
    "# em dados que ele não viu durante o treinamento (nossos dados de teste).\n",
    "\n",
    "print(\"\\n--- Realizando Previsões ---\")\n",
    "# Fazendo previsões nos dados de teste.\n",
    "# O método 'predict' retorna a classe prevista para cada amostra no X_test.\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Opcional: Obtendo as probabilidades de cada classe para cada previsão.\n",
    "# O método 'predict_proba' retorna as probabilidades de uma amostra pertencer a cada classe.\n",
    "# Por exemplo, [[0.1, 0.9], [0.8, 0.2]] significa que a primeira amostra tem 10% de chance\n",
    "# de ser da classe 0 e 90% da classe 1, e a segunda o oposto.\n",
    "y_proba = gnb.predict_proba(X_test)\n",
    "print(\"Exemplo de Previsões (primeiras 5):\", y_pred[:5])\n",
    "print(\"Exemplo de Probabilidades (primeiras 5):\", y_proba[:5])\n",
    "\n",
    "\n",
    "# 4. Avaliação do Modelo\n",
    "# Após fazer as previsões, é crucial avaliar o desempenho do nosso modelo.\n",
    "# Usamos métricas de avaliação comparando as previsões (y_pred) com os rótulos reais (y_test).\n",
    "\n",
    "print(\"\\n--- Avaliação do Modelo ---\")\n",
    "\n",
    "# Acurácia: Proporção de previsões corretas em relação ao total de previsões.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia do Modelo: {accuracy:.4f}\")\n",
    "\n",
    "# Relatório de Classificação: Fornece métricas detalhadas para cada classe,\n",
    "# como Precisão (Precision), Recall e F1-Score.\n",
    "# - Precisão: Das amostras classificadas como positivas, quantas são realmente positivas?\n",
    "# - Recall: Das amostras realmente positivas, quantas foram corretamente identificadas?\n",
    "# - F1-Score: Média harmônica de precisão e recall.\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de Confusão: Uma tabela que mostra a contagem de previsões corretas e incorretas.\n",
    "# Ajuda a visualizar onde o modelo está errando.\n",
    "# Linhas: Classes Reais (True Labels)\n",
    "# Colunas: Classes Previstas (Predicted Labels)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualizando a Matriz de Confusão (opcional, mas recomendado)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Previsto 0', 'Previsto 1'],\n",
    "            yticklabels=['Real 0', 'Real 1'])\n",
    "plt.xlabel('Rótulo Previsto')\n",
    "plt.ylabel('Rótulo Verdadeiro')\n",
    "plt.title('Matriz de Confusão do Gaussian Naive Bayes')\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Visualização dos Dados e Limite de Decisão (Opcional) ---\n",
    "# Para modelos com 2 ou 3 características, podemos visualizar os dados e\n",
    "# como o algoritmo separa as classes.\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plotando os pontos de dados de treino\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color='blue', label='Treino - Classe 0', alpha=0.7)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color='red', label='Treino - Classe 1', alpha=0.7)\n",
    "\n",
    "# Plotando os pontos de dados de teste (com as previsões)\n",
    "plt.scatter(X_test[y_pred == 0, 0], X_test[y_pred == 0, 1], color='lightblue', marker='X', s=100, label='Teste - Previsto 0', alpha=0.8)\n",
    "plt.scatter(X_test[y_pred == 1, 0], X_test[y_pred == 1, 1], color='salmon', marker='X', s=100, label='Teste - Previsto 1', alpha=0.8)\n",
    "\n",
    "# Criando a grade para plotar o limite de decisão\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Prevendo as classes para cada ponto na grade\n",
    "Z = gnb.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotando o limite de decisão (contornos)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)\n",
    "\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.title('Separação de Classes com Gaussian Naive Bayes')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be1a88-f5d0-4a93-a57d-2826fdd2eaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
